# -*- coding: utf-8 -*-
"""Adam-30.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eFysS_DeMnINgqKpEIasGtDQ6dcDtKk1
"""

from google.colab import drive
drive.mount('/content/drive')

# === IMPORT LIBRARIES ===
import os
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import KFold
from sklearn.metrics import classification_report, confusion_matrix
from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array
from tensorflow.keras.applications import VGG16, ResNet50, DenseNet121
from tensorflow.keras.applications.vgg16 import preprocess_input as vgg_preprocess
from tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocess
from tensorflow.keras.applications.densenet import preprocess_input as densenet_preprocess
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
import tensorflow as tf
from glob import glob
from PIL import Image
from sklearn.utils import shuffle
from google.colab import files # Untuk Google Colab

# === CONFIG ===
# Pastikan path ini sesuai dengan lokasi dataset Anda di Google Drive atau lokal
data_dir = '/content/drive/MyDrive/tugas_akhir/batik'
img_size = (224, 224)
batch_size = 32
num_classes = 6 # Sesuaikan dengan jumlah kelas motif batik Anda
epoch = 30 # Jumlah epoch yang lebih sedikit untuk demonstrasi mahasiswa

# Buat direktori untuk menyimpan model (jika diperlukan, meskipun sekarang output langsung)
os.makedirs("saved_models", exist_ok=True)

def load_dataset(data_dir):
    """Memuat dataset gambar dari direktori."""
    class_names = sorted(os.listdir(data_dir))
    print(f"Kelas yang ditemukan: {class_names}")

    images = []
    labels = []

    for idx, class_name in enumerate(class_names):
        class_folder = os.path.join(data_dir, class_name)
        image_files = glob(class_folder + '/*.jpg') + glob(class_folder + '/*.png')

        print(f"Kelas '{class_name}': {len(image_files)} gambar")

        for img_file in image_files:
            try:
                img = Image.open(img_file).convert('RGB').resize(img_size)
                images.append(np.array(img))
                labels.append(idx)
            except Exception as e:
                print(f"Error memuat {img_file}: {e}")

    print(f"Total gambar yang dimuat: {len(images)}")

    return np.array(images), np.array(labels), class_names

# Muat data
X, y_numeric, class_names = load_dataset(data_dir)
y = to_categorical(y_numeric, num_classes)

# Acak data
X, y, y_numeric = shuffle(X, y, y_numeric, random_state=42)

print("\nData berhasil dimuat dan diacak.")

kf = KFold(n_splits=5, shuffle=True, random_state=42)

import albumentations as A
import matplotlib.pyplot as plt

# Mean & Std dari ImageNet
IMAGENET_MEAN = [0.485, 0.456, 0.406]
IMAGENET_STD = [0.229, 0.224, 0.225]

# Definisi augmentasi untuk train
def get_train_transforms():
    return A.Compose([
        A.Resize(224, 224),
        A.HorizontalFlip(p=1),
        A.RandomBrightnessContrast(p=0.2),
        A.ShiftScaleRotate(0.1, 0.1, 15, p=0.5),
        A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),
    ])

# Definisi augmentasi untuk validasi
def get_val_transforms():
    return A.Compose([
        A.Resize(224, 224),
        A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),
    ])

# Augmentasi khusus preview (tanpa normalisasi, prob 1.0 biar jelas)
def get_train_transforms_preview():
    return A.Compose([
        A.Resize(224, 224),
        A.HorizontalFlip(p=1),
        A.RandomBrightnessContrast(p=1),
        A.ShiftScaleRotate(0.1, 0.1, 15, p=1),
    ])

# ==== PREVIEW AUGMENTASI ====
# Ambil 1 sample gambar dari dataset penuh
sample_img = X[0]  # ambil gambar pertama
augmenter = get_train_transforms_preview()

# Buat 5 versi augmentasi random
fig, axes = plt.subplots(1, 6, figsize=(20, 5))

# Gambar asli
axes[0].imshow(sample_img.astype("uint8"))
axes[0].set_title("Asli")
axes[0].axis("off")

# Gambar hasil augmentasi
for i in range(5):
    augmented = augmenter(image=sample_img)
    aug_img = augmented["image"]

    # Normalisasi kembali ke range [0,255] agar bisa ditampilkan
    aug_vis = ((aug_img - aug_img.min()) / (aug_img.max() - aug_img.min()) * 255).astype("uint8")

    axes[i+1].imshow(aug_vis)
    axes[i+1].set_title(f"Aug {i+1}")
    axes[i+1].axis("off")

plt.suptitle("Preview Augmentasi Data", fontsize=16)
plt.show()

from tensorflow.keras.utils import Sequence

class CustomDataGenerator(Sequence):
    def __init__(self, images, labels, batch_size=32, transform=None, shuffle=True):
        self.images = images
        self.labels = labels
        self.batch_size = batch_size
        self.transform = transform
        self.shuffle = shuffle
        self.indices = np.arange(len(images))
        self.on_epoch_end()

    def __len__(self):
        return int(np.ceil(len(self.images) / self.batch_size))

    def __getitem__(self, idx):
        batch_idx = self.indices[idx*self.batch_size:(idx+1)*self.batch_size]
        batch_images = []
        for i in batch_idx:
            aug = self.transform(image=self.images[i])
            batch_images.append(aug["image"])
        return np.array(batch_images), self.labels[batch_idx]

    def on_epoch_end(self):
        if self.shuffle: np.random.shuffle(self.indices)

print("Data augmenter siap.")

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.applications import VGG16, ResNet50, DenseNet121
from tensorflow.keras.applications.vgg16 import preprocess_input as vgg_preprocess
from tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocess
from tensorflow.keras.applications.densenet import preprocess_input as densenet_preprocess

def build_model(base_model_fn, num_classes):
    """Bangun model CNN transfer learning dengan classifier sederhana."""
    base_model = base_model_fn(
        include_top=False,
        input_shape=(224, 224, 3),
        weights='imagenet'
    )
    base_model.trainable = False

    x = GlobalAveragePooling2D()(base_model.output)
    x = Dense(256, activation='relu')(x)
    x = Dropout(0.5)(x)
    out = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=base_model.input, outputs=out)

    model.compile(
        optimizer=Adam(learning_rate=1e-4),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )

    return model

print("Model builder siap.")

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix

def display_classification_report(y_true, y_pred_classes, target_names):
    """Menampilkan classification report langsung di output cell."""
    print("\nClassification Report:")
    print(classification_report(y_true, y_pred_classes, target_names=target_names))

def plot_and_display_confusion_matrix(cm, title, class_names):
    """Memplot dan menampilkan confusion matrix langsung di output cell."""
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names,
                yticklabels=class_names)
    plt.title(title)
    plt.ylabel('Label Sebenarnya')
    plt.xlabel('Label Prediksi')
    plt.tight_layout()
    plt.show()

def plot_training_history_per_fold(model_name, fold_histories, best_fold=None):
    """Memplot riwayat training per fold dengan plot akurasi dan loss terpisah."""
    n_folds = len(fold_histories)
    n_rows = n_folds
    n_cols = 2  # Kolom 1: Accuracy, Kolom 2: Loss

    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))
    fig.suptitle(f'{model_name.upper()} - Riwayat Training per Fold', fontsize=20, y=0.99)

    if n_folds == 1:
        axes = np.expand_dims(axes, axis=0)

    for fold, history in enumerate(fold_histories):
        epochs = range(1, len(history['accuracy']) + 1)

        is_best = (fold+1) == best_fold
        color = 'red' if is_best else 'blue'
        linewidth = 3 if is_best else 2
        title_suffix = " (TERBAIK)" if is_best else ""

        # Akurasi
        ax_acc = axes[fold, 0]
        ax_acc.plot(epochs, history['accuracy'], 'b-', linewidth=linewidth,
                    label='Akurasi Train', alpha=0.8 if not is_best else 1.0)
        ax_acc.plot(epochs, history['val_accuracy'], 'g-', linewidth=linewidth,
                    label='Akurasi Validasi', alpha=0.8 if not is_best else 1.0)
        ax_acc.set_title(f'Fold {fold+1} - Akurasi{title_suffix}', fontsize=14, color=color)
        ax_acc.set_xlabel('Epochs', fontsize=12)
        ax_acc.set_ylabel('Akurasi', fontsize=12)
        ax_acc.set_ylim(0, 1.05)
        ax_acc.grid(True, alpha=0.3)
        ax_acc.legend(loc='lower right')

        # Loss
        ax_loss = axes[fold, 1]
        ax_loss.plot(epochs, history['loss'], 'r-', linewidth=linewidth,
                     label='Loss Train', alpha=0.8 if not is_best else 1.0)
        ax_loss.plot(epochs, history['val_loss'], 'm-', linewidth=linewidth,
                     label='Loss Validasi', alpha=0.8 if not is_best else 1.0)
        ax_loss.set_title(f'Fold {fold+1} - Loss{title_suffix}', fontsize=14, color=color)
        ax_loss.set_xlabel('Epochs', fontsize=12)
        ax_loss.set_ylabel('Loss', fontsize=12)

        max_loss = max(max(history['loss']), max(history['val_loss'])) * 1.1
        ax_loss.set_ylim(0, max_loss)
        ax_loss.grid(True, alpha=0.3)
        ax_loss.legend(loc='upper right')

    plt.tight_layout(rect=[0, 0.01, 1, 0.97])
    plt.subplots_adjust(hspace=0.3)
    plt.show()

print("Fungsi evaluasi dan visualisasi siap untuk menampilkan hasil.")

# ========================================
# HITUNG PEMBAGIAN DATA K-FOLD (sekali saja)
# ========================================
kf = KFold(n_splits=5, shuffle=True, random_state=42)
first_split = list(kf.split(X, y_numeric))[0]
train_idx, val_idx = first_split
train_count = len(train_idx)
val_count = len(val_idx)
total_count = train_count + val_count

print("\nPembagian Data K-Fold")
print(f"Jumlah Fold   : {kf.get_n_splits()}")
print(f"Training data : {train_count} gambar")
print(f"Validation    : {val_count} gambar")
print(f"Total data    : {total_count} gambar")


# ========================================
# FUNGSI TRAINING & EVALUASI
# ========================================
def train_and_evaluate(model_name, cv_folds=5):
    base_model_fn, _ = models_dict[model_name]
    best_acc, best_fold, best_model = 0, 0, None
    accs = []
    fold_histories = []

    kf = KFold(n_splits=cv_folds, shuffle=True, random_state=42)

    print(f"\n=== Melatih model: {model_name.upper()} ===")

    for fold, (train_idx, val_idx) in enumerate(kf.split(X, y_numeric)):
        print(f"\n{'='*40}\nFold {fold+1}/{cv_folds}")

        X_train, y_train = X[train_idx], y[train_idx]
        X_val, y_val = X[val_idx], y[val_idx]

        # Data generator
        train_gen = CustomDataGenerator(
            X_train, y_train,
            batch_size=batch_size,
            transform=get_train_transforms(),
            shuffle=True
        )
        val_gen = CustomDataGenerator(
            X_val, y_val,
            batch_size=batch_size,
            transform=get_val_transforms(),
            shuffle=False
        )

        # Build & train model
        model = build_model(base_model_fn, num_classes)
        history = model.fit(train_gen, validation_data=val_gen, epochs=epoch, verbose=1)
        fold_histories.append(history.history)

        # Evaluasi
        val_loss, val_acc = model.evaluate(val_gen, verbose=0)
        accs.append(val_acc)
        print(f"Akurasi val: {val_acc:.4f}")

        if val_acc > best_acc:
            best_acc, best_fold, best_model = val_acc, fold+1, model

        # Classification report & confusion matrix
        y_pred = np.argmax(model.predict(val_gen), axis=1)
        y_true = np.argmax(y_val, axis=1)
        display_classification_report(y_true, y_pred, class_names)
        cm = confusion_matrix(y_true, y_pred)
        plot_and_display_confusion_matrix(cm, f"{model_name.upper()} Fold {fold+1}", class_names)

        tf.keras.backend.clear_session()

    print(f"\n{'='*40}")
    print(f"Rata-rata Akurasi Validasi {model_name.upper()}: {np.mean(accs):.4f}")
    print(f"Akurasi Validasi Terbaik {model_name.upper()}: {best_acc:.4f} (Fold {best_fold})")
    print(f"{'='*40}")

    if best_model:
        best_model.save(f"saved_models/{model_name}_best.h5")
        print(f"Model terbaik disimpan -> saved_models/{model_name}_best.h5")

    return np.mean(accs), best_acc, best_fold, fold_histories


print("Fungsi training dan evaluasi siap.")

from tensorflow.keras.applications import VGG16, ResNet50, DenseNet121
from tensorflow.keras.applications.vgg16 import preprocess_input as vgg_preprocess
from tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocess
from tensorflow.keras.applications.densenet import preprocess_input as densenet_preprocess

models_dict = {
    'vgg16': (VGG16, vgg_preprocess),
    'resnet50': (ResNet50, resnet_preprocess),
    'densenet121': (DenseNet121, densenet_preprocess)
}

results = {}

# === EKSEKUSI MODEL VGG16 ===
model_name = 'vgg16'
print(f"\n{'#'*50}")
print(f"MEMPROSES MODEL: {model_name.upper()}")
print(f"{'#'*50}")

mean_acc, best_acc, best_fold, fold_hists = train_and_evaluate(model_name, cv_folds=5)
results[model_name] = {
    'mean_acc': mean_acc,
    'best_acc': best_acc,
    'best_fold': best_fold
}
plot_training_history_per_fold(model_name, fold_hists, best_fold)

# === EKSEKUSI MODEL RESNET50 ===
model_name = 'resnet50'
print(f"\n{'#'*50}")
print(f"MEMPROSES MODEL: {model_name.upper()}")
print(f"{'#'*50}")

mean_acc, best_acc, best_fold, fold_hists = train_and_evaluate(model_name, cv_folds=5)
results[model_name] = {
    'mean_acc': mean_acc,
    'best_acc': best_acc,
    'best_fold': best_fold
}
plot_training_history_per_fold(model_name, fold_hists, best_fold)

# === EKSEKUSI MODEL DENSENET121 ===

model_name = 'densenet121'
print(f"\n{'#'*50}")
print(f"MEMPROSES MODEL: {model_name.upper()}")
print(f"{'#'*50}")

mean_acc, best_acc, best_fold, fold_hists = train_and_evaluate(model_name, cv_folds=5)
results[model_name] = {
    'mean_acc': mean_acc,
    'best_acc': best_acc,
    'best_fold': best_fold
}
plot_training_history_per_fold(model_name, fold_hists, best_fold)

# Sudah ada variabel 'results' berisi semua hasil
# Tidak perlu bikin ulang results = { ... }

print("\n" + "="*60)
print("üîç PERBANDINGAN AKHIR MODEL")
print("="*60)
print(f"{'Model':<12} | {'Rata-rata Akurasi':<18} | {'Akurasi Terbaik':<18} | {'Fold Terbaik'}")
print("-"*70)

best_mean_model = None
best_acc_model = None
best_mean_val = -1
best_acc_val = -1

for model, metrics in results.items():
    if metrics is not None:
        mean_acc = metrics['mean_acc']
        best_acc = metrics['best_acc']

        print(f"{model.upper():<12} | {mean_acc:.4f}             | {best_acc:.4f}             | {metrics['best_fold']}")

        if mean_acc > best_mean_val:
            best_mean_val = mean_acc
            best_mean_model = model

        if best_acc > best_acc_val:
            best_acc_val = best_acc
            best_acc_model = model
    else:
        print(f"{model.upper():<12} | {'N/A':<18} | {'N/A':<18} | {'N/A'}")

print("-"*70)

if best_mean_model:
    print(f"\n‚úÖ Model dengan RATA-RATA akurasi tertinggi: {best_mean_model.upper()} ({best_mean_val:.4f})")

if best_acc_model:
    print(f"‚úÖ Model dengan AKURASI tertinggi di satu fold: {best_acc_model.upper()} ({best_acc_val:.4f})")

if best_mean_val >= 0.80:
    print("\nüéØ Target akurasi rata-rata (> 80%) berhasil dicapai.")
else:
    print("\n‚ö†Ô∏è Akurasi rata-rata belum optimal. Pertimbangkan:")
    print("   - Menambah jumlah epoch")
    print("   - Teknik augmentasi yang lebih beragam")
    print("   - Fine-tuning beberapa layer model")

import numpy as np
import matplotlib.pyplot as plt
import cv2
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.preprocessing.image import img_to_array
from tensorflow.keras.applications.vgg16 import preprocess_input as vgg_preprocess
from tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocess
from tensorflow.keras.applications.densenet import preprocess_input as densenet_preprocess
from tensorflow.keras.preprocessing.image import array_to_img

# === LOAD MODEL TERBAIK ===
vgg_model = load_model("saved_models/vgg16_best.h5")
resnet_model = load_model("saved_models/resnet50_best.h5")
densenet_model = load_model("saved_models/densenet121_best.h5")

print("‚úÖ Semua model terbaik berhasil dimuat.")

# =====================================================
# FUNGSI: Visualisasi Feature Map
# =====================================================
def visualize_feature_maps(model, img_array, layer_name, preprocess_fn, max_channels=6):
    """Visualisasi beberapa channel feature map dari suatu layer."""
    # Preprocess image
    img_processed = np.expand_dims(img_array, axis=0)
    img_processed = preprocess_fn(img_processed)

    # Ambil output layer tertentu
    intermediate_model = Model(inputs=model.input,
                               outputs=model.get_layer(layer_name).output)
    feature_maps = intermediate_model.predict(img_processed)

    # Plot hanya sebagian channel agar mudah dibaca
    fig, axes = plt.subplots(1, max_channels, figsize=(20, 5))
    for i in range(max_channels):
        axes[i].imshow(feature_maps[0, :, :, i], cmap='viridis')
        axes[i].axis('off')
    plt.suptitle(f"Feature maps dari layer {layer_name}", fontsize=16)
    plt.show()

# =====================================================
# FUNGSI: Grad-CAM
# =====================================================
import tensorflow as tf

def make_gradcam_heatmap(img_array, model, last_conv_layer_name, preprocess_fn, pred_index=None):
    """Membuat heatmap Grad-CAM untuk citra tunggal."""
    img_processed = np.expand_dims(img_array, axis=0)
    img_processed = preprocess_fn(img_processed)

    grad_model = Model(
        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]
    )

    with tf.GradientTape() as tape:
        conv_outputs, predictions = grad_model(img_processed)
        if pred_index is None:
            pred_index = tf.argmax(predictions[0])
        class_channel = predictions[:, pred_index]

    grads = tape.gradient(class_channel, conv_outputs)
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))

    conv_outputs = conv_outputs[0]
    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)
    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)
    return heatmap.numpy()

def display_gradcam(img_array, model, preprocess_fn, last_conv_layer):
    """Menampilkan hasil Grad-CAM overlay di atas citra asli."""
    # Buat heatmap
    heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer, preprocess_fn)

    # Ubah array ke format OpenCV
    img = img_array.astype(np.uint8)
    img = cv2.resize(img, (224,224))
    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))
    heatmap = np.uint8(255 * heatmap)
    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)

    # Overlay heatmap ke gambar asli
    superimposed_img = cv2.addWeighted(img, 0.6, heatmap, 0.4, 0)
    plt.imshow(superimposed_img[:,:,::-1])
    plt.axis("off")
    plt.title("Grad-CAM Overlay")
    plt.show()

# =====================================================
# AMBIL SAMPLE GAMBAR DARI DATASET
# =====================================================
# Misalnya kamu masih punya X dan y_numeric dari proses training
# Ambil 1 gambar dari dataset (contoh index ke-15)
sample_idx = 15
sample_img = X[sample_idx]   # citra asli ukuran (224,224,3)
true_label = y_numeric[sample_idx]
print(f"Index {sample_idx} - Label asli: {true_label}")

# === VGG16 ===
print("=== VGG16 ===")
visualize_feature_maps(vgg_model, sample_img, "block1_conv2", vgg_preprocess)
display_gradcam(sample_img, vgg_model, vgg_preprocess, "block5_conv3")

# === ResNet50 ===
print("=== ResNet50 ===")
# Visualisasi feature map dari block terakhir conv sebelum pooling
visualize_feature_maps(resnet_model, sample_img, "conv5_block3_out", resnet_preprocess)
# Grad-CAM dari layer terakhir conv yang sama
display_gradcam(sample_img, resnet_model, resnet_preprocess, "conv5_block3_out")

# === DenseNet121 ===
print("=== DenseNet121 ===")
# Visualisasi feature map dari layer terakhir conv sebelum classifier
visualize_feature_maps(densenet_model, sample_img, "conv5_block16_concat", densenet_preprocess)
display_gradcam(sample_img, densenet_model, densenet_preprocess, "conv5_block16_concat")